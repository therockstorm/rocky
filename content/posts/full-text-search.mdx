---
title: "Full Text Search: PostgreSQL vs Elasticsearch"
date: 2020-09-02
---

I started investigating full-text search options recently. The use-case is real-time search over key-value pairs where the keys are strings and the values are either strings, numbers, or dates. I want full-text search over keys and values and range queries over number and date values. The search company Algolia recommends and [end-to-end latency budget](https://blog.algolia.com/algolia-v-elasticsearch-latency/) of no more than 50ms and I'm expecting 1.5 million unique key-value pairs as a medium-term maximum.

## PostgreSQL

Since I'm already familiar with PostgreSQL, I started by seeing if it could meet these requirements. First, let's create a table,

```sql
CREATE TABLE IF NOT EXISTS search_idx
(
    id       BIGINT PRIMARY KEY GENERATED BY DEFAULT AS IDENTITY,
    key_str  TEXT NOT NULL,
    val_str  TEXT NOT NULL,
    val_int  INT,
    val_date TIMESTAMPTZ
);
```

Next, we'll seed this with semi-realistic data. Here's a script that can insert about 20,000 rows/second into tables that don't have indexes,

```javascript
const pgp = require("pg-promise")()
const faker = require("faker")

const Iterations = 150

const seedDb = async () => {
  const db = pgp({
    database: process.env.DATABASE_NAME,
    user: process.env.DATABASE_USER,
    password: process.env.DATABASE_PASSWORD,
    max: 30
  })
  const columns = new pgp.helpers.ColumnSet(
    ["key_str", "val_str", "val_int", "val_date"],
    { table: "search_idx" }
  )

  const getNextData = (_, pageIdx) =>
    Promise.resolve(
      pageIdx > Iterations - 1
        ? null
        : Array.from(Array(10000)).map(() => ({
            key_str: `${faker.lorem.word()} ${faker.lorem.word()}`,
            val_str: faker.lorem.words(),
            val_int: Math.floor(faker.random.float()),
            val_date: faker.date.past()
          }))
    )

  console.log(
    await db.tx("seed-db", t =>
      t.sequence(idx =>
        getNextData(t, idx).then(data => {
          if (data) return t.none(pgp.helpers.insert(data, columns))
        })
      )
    )
  )
}

seedDb()
```

Now that the 1.5M rows of data is loaded, we'll add the full-text search GIN indexes ([details](https://www.postgresql.org/docs/current/textsearch.html)),

```sql
CREATE INDEX search_idx_key_str_idx ON search_idx
    USING GIN (to_tsvector('english'::regconfig, key_str));
CREATE INDEX search_idx_val_str_idx ON search_idx
    USING GIN (to_tsvector('english'::regconfig, val_str));
```

Note: If you add more data to the table after inserting the indexes, make sure to `VACUUM ANALYZE search_idx;` to update your table stats and improve your query plans.

Time to test performance. Run the following queries,

```sql
-- Prefix query across FTS columns
SELECT *
FROM search_idx
WHERE to_tsvector('english'::regconfig, key_str) @@ to_tsquery('english'::regconfig, 'qui:*')
  OR to_tsvector('english'::regconfig, val_str) @@ to_tsquery('english'::regconfig, 'qui:*');

-- Wildcard query on key
SELECT *
FROM search_idx
WHERE key_str ILIKE '%quis%';

-- Specific key and value(s) query
SELECT *
FROM search_idx
WHERE to_tsvector('english'::regconfig, key_str) @@ to_tsquery('english'::regconfig, 'quis')
  AND (to_tsvector('english'::regconfig, val_str) @@ to_tsquery('english'::regconfig, 'nulla')
    OR (to_tsvector('english'::regconfig, val_str) @@ to_tsquery('english'::regconfig, 'velit')));

-- Contrived range query, one field wouldn't have both val_int and val_date populated
SELECT *
FROM search_idx
WHERE to_tsvector('english'::regconfig, key_str) @@ to_tsquery('english'::regconfig, 'quis')
  AND val_int > 1000
  AND val_date > '2020-01-01';
```

Add `EXPLAIN` in front of any query to ensure it's using the index rather than doing a full table scan. Adding `EXPLAIN ANALYZE` in front will give you timing information, however, [as noted in the documentation](https://www.postgresql.org/docs/current/sql-explain.html), this adds overhead and can sometimes take significantly longer than executing the query normally.

I consistently get ~120ms on my MacBook Pro (2.4 GHz 8-Core i9, 32 GB RAM) on the first query, which will likely be the most common. This is well over the 50ms threshold.

## Elasticsearch

I tested Elasticsearch via the docker image. Start it with `docker run -d -p 9200:9200 -p 9300:9300 -e "discovery.type=single-node" docker.elastic.co/elasticsearch/elasticsearch:7.9.0`. Run `curl -sX GET "localhost:9200/_cat/health?v&pretty" -H "Accept: application/json"` to ensure the status is `green`.

Next, we'll seed the index using a couple of scripts. The first creates a file with each line a semi-realistic document,

```javascript
const faker = require("faker")
const { writeFileSync } = require("fs")

const Iterations = 1

writeFileSync(
  "./dataset.ndjson",
  Array.from(Array(Iterations))
    .map(() =>
      JSON.stringify({
        key: `${faker.lorem.word()} ${faker.lorem.word()}`,
        val: faker.lorem.words(),
        valInt: Math.floor(faker.random.float()),
        valDate: faker.date.past()
      })
    )
    .join("\n")
)
```

Now that the ~150MB file exists, we'll seed the index,

```javascript
const { createReadStream } = require("fs")
const split = require("split2")
const { Client } = require("@elastic/elasticsearch")

const Index = "search-idx"

const seedIndex = async () => {
  const client = new Client({ node: "http://localhost:9200" })
  console.log(
    await client.helpers.bulk({
      datasource: createReadStream("./dataset.ndjson").pipe(split()),
      onDocument(doc) {
        return { index: { _index: Index } }
      },
      onDrop(doc) { b.abort() }
    })
  )
}

seedIndex()
```

Note: this script is fine for testing purposes, but in production, follow best practices on [sizing bulk requests](https://www.elastic.co/guide/en/elasticsearch/guide/2.x/indexing-performance.html#_using_and_sizing_bulk_requests) and [using multiple threads](https://www.elastic.co/guide/en/elasticsearch/reference/current/tune-for-indexing-speed.html#multiple-workers-threads).

Now we can run some queries,


```shell
curl -sX GET "localhost:9200/search-idx/_search?pretty" \
-H 'Content-Type: application/json' \
-d'
{
  "query": {
    "simple_query_string" : {
      "query": "\"repellat sunt\" -quis",
      "fields": ["valStr", "keyStr"],
      "default_operator": "and"
    }
  }
}
'
```

Once the index is warm, I get ~8-20ms or 6x faster than PostgreSQL. So it seems like the added overhead of maintaining an Elasticsearch cluster may be worth it to get the performance I'm after.
